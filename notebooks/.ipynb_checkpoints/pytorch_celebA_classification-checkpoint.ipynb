{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# WELCOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import here\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.pytorch_cl_vae.model import ResNet18Classifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# params here\n",
    "params = {\n",
    "    'impath' : \"C:\\Study\\classifying_autoencoders\\data\\input\\img_align_celeba\\img_align_celeba\",\n",
    "    'attpath' : \"C:\\Study\\classifying_autoencoders\\data\\input/list_attr_celeba.txt\",\n",
    "    'attributes': ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'],\n",
    "    'classes_dim': [2,2,2,2,2],\n",
    "    'learning_rate': 5e-4,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 128,\n",
    "    'image_size': 224\n",
    "}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n"
     ]
    }
   ],
   "source": [
    "from src.loader import get_loader\n",
    "from src.loader import chew\n",
    "\n",
    "data_loader = get_loader(params['impath'],\n",
    "                  params['attpath'],\n",
    "                  params['attributes'],\n",
    "                  batch_size=params['batch_size'],\n",
    "                  image_size=params['image_size'])\n",
    "# xs, ys = next(iter(dset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Create Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "optimizers = []\n",
    "for idx, (name, classes_dim) in enumerate(zip(params['attributes'], params['classes_dim'])):\n",
    "    classifier = ResNet18Classifier(label_dim=params['classes_dim'][idx]).to(device)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=params['learning_rate']).to(device)\n",
    "    \n",
    "    classifiers.append(classifier)\n",
    "    optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2]) torch.Size([128, 2])\n",
      "torch.Size([128, 2]) torch.Size([128, 2])\n",
      "torch.Size([128, 2]) torch.Size([128, 2])\n",
      "torch.Size([128, 2]) torch.Size([128, 2])\n",
      "torch.Size([128, 2]) torch.Size([128, 2])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_step_i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-12a483305f7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m         print(\"\\r|progress: {:.2f}% | train step: {} | losses: {} | accuracies: {} |\"\n\u001b[0;32m     36\u001b[0m               \" | w_dkl loss: {:.4f} | class_accuracy: {:.4f} |\".format(\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[1;36m100.\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mtrain_step_i\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_train_samples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m                   loss_dict, acc_dict), end='')\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrain_step_i\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_step_i' is not defined"
     ]
    }
   ],
   "source": [
    "num_train_samples = len(data_loader)\n",
    "\n",
    "train_step_i = 0\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "for epoch in range(params['num_epochs']):\n",
    "    for x_batch, ws_batch in data_loader:\n",
    "        ws_batch = chew(ws_batch)\n",
    "        train_step_i += 1\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for label_index, (classifier, optim) in enumerate(zip(classifiers, optimizers)):\n",
    "            x_batch = x_batch.to(device)\n",
    "            optim.zero_grad()\n",
    "#             classifier.train()\n",
    "            predictions, _ = classifier(x_batch)\n",
    "            w_true = ws_batch[label_index].long()\n",
    "#             w_true = w_true.max(1)[1]\n",
    "            labels = w_true.max(1)[1].squeeze().to(device)\n",
    "            print(predictions.shape, w_true.shape)\n",
    "            loss = CrossEntropyLoss()(predictions, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss)\n",
    "            labels = w_true.max(1)[1].squeeze()\n",
    "            labels_predict = predictions.max(1)[1].squeeze()\n",
    "            acc = accuracy_score(labels, labels_predict)\n",
    "            accuracies.append(acc)\n",
    "            \n",
    "        train_losses.append(losses)\n",
    "        train_accuracies.append(accuracies)\n",
    "        loss_dict = dict(zip(params['attributes'], losses))\n",
    "        acc_dict = dict(zip(params['attributes'], accuracies))\n",
    "\n",
    "        print(\"\\r|progress: {:.2f}% | train step: {} | losses: {} | accuracies: {} |\"\n",
    "              \" | w_dkl loss: {:.4f} | class_accuracy: {:.4f} |\".format(\n",
    "            100.* train_step_i / (num_train_samples // params['batch_size'] * params['num_epochs']), train_step_i,\n",
    "                  loss_dict, acc_dict), end='')\n",
    "        if train_step_i % 100 == 0:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.array(train_losses)\n",
    "train_accuracies = np.array(train_accuracies)\n",
    "\n",
    "for i, attr in enumerate(params['attributes']):\n",
    "    plt.plot(train_losses[:, i])\n",
    "    plt.title(attr)\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(train_accuracies[:, i])\n",
    "    plt.title(attr)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
